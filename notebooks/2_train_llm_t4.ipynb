{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T4-OPT: Train LLM with QLoRA\n",
        "\n",
        "This notebook demonstrates QLoRA fine-tuning on a T4 GPU.\n",
        "\n",
        "## Steps:\n",
        "1. Load and prepare dataset\n",
        "2. Configure QLoRA training\n",
        "3. Train the model\n",
        "4. Save checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/t4opt')\n",
        "\n",
        "from training.qlora import QLoRATrainer, QLoRAConfig\n",
        "from training.dataset import DatasetManager\n",
        "from utils.memory import MemoryManager\n",
        "from utils.config import Config\n",
        "\n",
        "# Check memory\n",
        "MemoryManager.print_memory_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for T4\n",
        "config = QLoRAConfig(\n",
        "    model_name=\"microsoft/phi-2\",  # or \"google/gemma-2b-it\"\n",
        "    output_dir=\"./checkpoints/phi-2-qlora\",\n",
        "    max_seq_length=1024,\n",
        "    micro_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    lora_r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    use_gradient_checkpointing=True,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "for key, value in config.__dict__.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset_manager = DatasetManager()\n",
        "dataset_info = dataset_manager.load_dataset(\n",
        "    dataset_name=\"alpaca\",\n",
        "    max_samples=1000  # Limit for T4\n",
        ")\n",
        "\n",
        "print(f\"Dataset loaded: {dataset_info['num_samples']} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = QLoRATrainer(config=config)\n",
        "\n",
        "# Load model\n",
        "model, tokenizer = trainer.load_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize dataset\n",
        "tokenized_dataset = dataset_manager.tokenize_dataset(\n",
        "    dataset_info['dataset'],\n",
        "    tokenizer,\n",
        "    max_length=config.max_seq_length\n",
        ")\n",
        "\n",
        "print(f\"Tokenized dataset: {len(tokenized_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "training_result = trainer.train(tokenized_dataset)\n",
        "\n",
        "print(\"\\nTraining Results:\")\n",
        "print(f\"  Final Loss: {training_result['train_loss']:.4f}\")\n",
        "print(f\"  Training Time: {training_result['train_runtime']:.2f} seconds\")\n",
        "print(f\"  Samples/sec: {training_result['train_samples_per_second']:.2f}\")\n",
        "print(f\"  Output Directory: {training_result['output_dir']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check memory after training\n",
        "MemoryManager.print_memory_summary()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
