{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T4-OPT: Quantize Model\n",
        "\n",
        "This notebook demonstrates model quantization after training.\n",
        "\n",
        "## Steps:\n",
        "1. Load trained model with LoRA\n",
        "2. Merge LoRA adapters\n",
        "3. Quantize to INT8 or AWQ\n",
        "4. Export quantized model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/t4opt')\n",
        "\n",
        "from quant.merge_lora import merge_lora_weights\n",
        "from quant.quant_int8 import quantize_to_int8\n",
        "from quant.quant_awq import quantize_to_awq\n",
        "from utils.memory import MemoryManager\n",
        "\n",
        "MemoryManager.print_memory_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths (adjust based on your training output)\n",
        "base_model_path = \"microsoft/phi-2\"\n",
        "lora_path = \"./checkpoints/phi-2-qlora\"\n",
        "merged_output = \"./merged_models/phi-2-merged\"\n",
        "\n",
        "print(f\"Base model: {base_model_path}\")\n",
        "print(f\"LoRA path: {lora_path}\")\n",
        "print(f\"Merged output: {merged_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapters\n",
        "merge_result = merge_lora_weights(\n",
        "    base_model_path=base_model_path,\n",
        "    lora_path=lora_path,\n",
        "    output_path=merged_output\n",
        ")\n",
        "\n",
        "print(f\"Merged model size: {merge_result['model_size_mb']:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantize to INT8\n",
        "int8_result = quantize_to_int8(\n",
        "    model_path=merged_output,\n",
        "    context={\"output_path\": \"./quantized_models/phi-2-int8\"}\n",
        ")\n",
        "\n",
        "print(f\"Original size: {int8_result['original_size_mb']:.2f} MB\")\n",
        "print(f\"Quantized size: {int8_result['quantized_size_mb']:.2f} MB\")\n",
        "print(f\"Size reduction: {int8_result['size_reduction_percent']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Quantize to AWQ (4-bit)\n",
        "# Note: Requires autoawq library\n",
        "try:\n",
        "    awq_result = quantize_to_awq(\n",
        "        model_path=merged_output,\n",
        "        context={\"output_path\": \"./quantized_models/phi-2-awq\"}\n",
        "    )\n",
        "    print(\"AWQ quantization complete\")\n",
        "except Exception as e:\n",
        "    print(f\"AWQ quantization failed: {e}\")\n",
        "    print(\"Using NF4 fallback instead\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
