{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T4-OPT: Quantize Model\n",
        "\n",
        "This notebook demonstrates model quantization after training.\n",
        "\n",
        "## Steps:\n",
        "1. Load trained model with LoRA\n",
        "2. Merge LoRA adapters\n",
        "3. Quantize to INT8 or AWQ\n",
        "4. Export quantized model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/t4opt')\n",
        "\n",
        "from quant.merge_lora import merge_lora_weights\n",
        "from quant.quant_int8 import quantize_to_int8\n",
        "from quant.quant_awq import quantize_to_awq\n",
        "from utils.memory import MemoryManager\n",
        "from utils.checkpoint_utils import print_checkpoint_info, check_drive_checkpoints\n",
        "\n",
        "# Optional: Mount Google Drive if checkpoints are saved there\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "MemoryManager.print_memory_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths (adjust based on your training output)\n",
        "# IMPORTANT: If you saved to Google Drive, use Drive paths!\n",
        "# Example: lora_path = \"/content/drive/MyDrive/t4opt_checkpoints/phi-2-qlora\"\n",
        "\n",
        "base_model_path = \"microsoft/phi-2\"\n",
        "lora_path = \"./checkpoints/phi-2-qlora\"  # ‚ö†Ô∏è Check if this exists after session crash!\n",
        "merged_output = \"./merged_models/phi-2-merged\"\n",
        "\n",
        "# Check if checkpoints exist\n",
        "print(\"Checking for checkpoints...\")\n",
        "print_checkpoint_info(lora_path)\n",
        "\n",
        "# Also check Drive if mounted\n",
        "drive_info = check_drive_checkpoints()\n",
        "if drive_info[\"drive_mounted\"] and drive_info[\"checkpoints\"]:\n",
        "    print(\"\\nüí° Found checkpoints in Drive! Update lora_path to use Drive path.\")\n",
        "    for name in drive_info[\"checkpoints\"].keys():\n",
        "        print(f\"   Example: lora_path = '/content/drive/MyDrive/t4opt_checkpoints/{name}'\")\n",
        "\n",
        "print(f\"\\nBase model: {base_model_path}\")\n",
        "print(f\"LoRA path: {lora_path}\")\n",
        "print(f\"Merged output: {merged_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapters\n",
        "merge_result = merge_lora_weights(\n",
        "    base_model_path=base_model_path,\n",
        "    lora_path=lora_path,\n",
        "    output_path=merged_output\n",
        ")\n",
        "\n",
        "print(f\"Merged model size: {merge_result['model_size_mb']:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantize to INT8\n",
        "int8_result = quantize_to_int8(\n",
        "    model_path=merged_output,\n",
        "    context={\"output_path\": \"./quantized_models/phi-2-int8\"}\n",
        ")\n",
        "\n",
        "print(f\"Original size: {int8_result['original_size_mb']:.2f} MB\")\n",
        "print(f\"Quantized size: {int8_result['quantized_size_mb']:.2f} MB\")\n",
        "print(f\"Size reduction: {int8_result['size_reduction_percent']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Quantize to AWQ (4-bit)\n",
        "# Note: Requires autoawq library\n",
        "try:\n",
        "    awq_result = quantize_to_awq(\n",
        "        model_path=merged_output,\n",
        "        context={\"output_path\": \"./quantized_models/phi-2-awq\"}\n",
        "    )\n",
        "    print(\"AWQ quantization complete\")\n",
        "except Exception as e:\n",
        "    print(f\"AWQ quantization failed: {e}\")\n",
        "    print(\"Using NF4 fallback instead\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
