{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T4-OPT: Quantize Model\n",
        "\n",
        "This notebook demonstrates model quantization after training.\n",
        "\n",
        "## Steps:\n",
        "1. Load trained model with LoRA\n",
        "2. Merge LoRA adapters\n",
        "3. Quantize to INT8 or AWQ (with GPU acceleration!)\n",
        "4. Export quantized model\n",
        "\n",
        "## GPU Utilization\n",
        "- **GPU-accelerated quantization** (default): Uses bitsandbytes for 8-bit quantization on GPU - utilizes GPU fully!\n",
        "- **CPU-based quantization**: Uses PyTorch's quantize_dynamic on CPU - slower but more memory-efficient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/t4opt')\n",
        "\n",
        "from quant.merge_lora import merge_lora_weights\n",
        "from quant.quant_int8 import quantize_to_int8\n",
        "from quant.quant_awq import quantize_to_awq\n",
        "from utils.memory import MemoryManager\n",
        "from utils.checkpoint_utils import print_checkpoint_info, check_drive_checkpoints\n",
        "\n",
        "\n",
        "MemoryManager.print_memory_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model_path = \"microsoft/phi-2\"\n",
        "lora_path = \"./checkpoints/phi-2-qlora\"  \n",
        "merged_output = \"./merged_models/phi-2-merged\"\n",
        "\n",
        "print(\"Checking for checkpoints...\")\n",
        "print_checkpoint_info(lora_path)\n",
        "\n",
        "drive_info = check_drive_checkpoints()\n",
        "if drive_info[\"drive_mounted\"] and drive_info[\"checkpoints\"]:\n",
        "    print(\"\\nFound checkpoints in Drive! Update lora_path to use Drive path.\")\n",
        "    for name in drive_info[\"checkpoints\"].keys():\n",
        "        print(f\" Example: lora_path = '/content/drive/MyDrive/t4opt_checkpoints/{name}'\")\n",
        "\n",
        "print(f\"\\nBase model: {base_model_path}\")\n",
        "print(f\"LoRA path: {lora_path}\")\n",
        "print(f\"Merged output: {merged_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Memory status after merge:\")\n",
        "MemoryManager.print_memory_summary()\n",
        "\n",
        "MemoryManager.clear_cache()\n",
        "print(\"\\nMemory cleared. Ready for quantization.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge_result = merge_lora_weights(\n",
        "    base_model_path=base_model_path,\n",
        "    lora_path=lora_path,\n",
        "    output_path=merged_output\n",
        ")\n",
        "\n",
        "print(f\"Merged model size: {merge_result['model_size_mb']:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "int8_result = quantize_to_int8(\n",
        "    model_path=merged_output,\n",
        "    context={\"output_path\": \"./quantized_models/phi-2-int8\"},\n",
        "    use_gpu=True  \n",
        ")\n",
        "\n",
        "print(f\"\\nQuantization complete!\")\n",
        "print(f\"Original size: {int8_result['original_size_mb']:.2f} MB\")\n",
        "print(f\"Quantized size: {int8_result['quantized_size_mb']:.2f} MB\")\n",
        "print(f\"Size reduction: {int8_result['size_reduction_percent']:.2f}%\")\n",
        "print(f\"Device used: {int8_result.get('device', 'cpu')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    awq_result = quantize_to_awq(\n",
        "        model_path=merged_output,\n",
        "        context={\"output_path\": \"./quantized_models/phi-2-awq\"}\n",
        "    )\n",
        "    print(\"AWQ quantization complete\")\n",
        "except Exception as e:\n",
        "    print(f\"AWQ quantization failed: {e}\")\n",
        "    print(\"Using NF4 fallback instead\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
