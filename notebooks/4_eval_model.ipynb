{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/t4opt')\n",
        "\n",
        "from eval.perplexity import PerplexityEvaluator\n",
        "from eval.benchmarks import BenchmarkRunner\n",
        "from eval.speed_test import SpeedTester\n",
        "from utils.memory import MemoryManager\n",
        "\n",
        "# Model path (adjust based on your model)\n",
        "model_path = \"./checkpoints/phi-2-qlora\"  # or quantized model path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perplexity evaluation\n",
        "perplexity_evaluator = PerplexityEvaluator(model_path=model_path)\n",
        "perplexity_result = perplexity_evaluator.evaluate(max_samples=50)\n",
        "\n",
        "print(f\"Perplexity: {perplexity_result['perplexity']:.4f}\")\n",
        "print(f\"Average Loss: {perplexity_result['average_loss']:.4f}\")\n",
        "print(f\"Total Tokens: {perplexity_result['total_tokens']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run benchmarks\n",
        "benchmark_runner = BenchmarkRunner(model_path=model_path)\n",
        "benchmark_results = benchmark_runner.run(benchmarks=[\"mmlu\", \"generation\"])\n",
        "\n",
        "print(\"Benchmark Results:\")\n",
        "for benchmark, results in benchmark_results.items():\n",
        "    print(f\"\\n{benchmark.upper()}:\")\n",
        "    if isinstance(results, dict):\n",
        "        for key, value in results.items():\n",
        "            if key != \"generations\":  # Skip full generation text\n",
        "                print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Speed test\n",
        "speed_tester = SpeedTester(model_path=model_path)\n",
        "latency_results = speed_tester.test_latency(num_runs=10)\n",
        "\n",
        "print(\"Latency Results:\")\n",
        "print(f\"  Average Latency: {latency_results['avg_latency_ms']:.2f} ms\")\n",
        "print(f\"  Tokens/Second: {latency_results['avg_tokens_per_second']:.2f}\")\n",
        "print(f\"  Device: {latency_results['device']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate evaluation report\n",
        "from agents.evaluator import EvalAgent\n",
        "\n",
        "eval_agent = EvalAgent()\n",
        "report_result = eval_agent.execute(\n",
        "    task=\"generate_report\",\n",
        "    context={\n",
        "        \"results\": {\n",
        "            \"perplexity\": perplexity_result[\"perplexity\"],\n",
        "            \"benchmarks\": {\n",
        "                \"mmlu_accuracy\": benchmark_results.get(\"mmlu\", {}).get(\"accuracy\", 0),\n",
        "                \"generation_avg_length\": benchmark_results.get(\"generation\", {}).get(\"avg_generation_length\", 0)\n",
        "            }\n",
        "        },\n",
        "        \"output_path\": \"./eval_report.txt\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Evaluation report generated!\")\n",
        "print(f\"Report saved to: {report_result.result['output_path']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
